{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pypokerengine.players import BasePokerPlayer\n",
    "from pypokerengine.utils.card_utils import Card, Deck\n",
    "from pypokerengine.api.game import setup_config, start_poker\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import scipy\n",
    "import scipy.signal\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../scripts/')\n",
    "\n",
    "import PlayerModels as pm\n",
    "from MyEmulator import MyEmulator\n",
    "from util import *\n",
    "\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "from random import choice\n",
    "from time import sleep\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "# Discounting function used to calculate discounted returns.\n",
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "#Used to initialize weights for policy and value output layers\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self, a_size, scope, trainer, h_size=64, is_train=True):\n",
    "        self.h_size = h_size\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        with tf.variable_scope(scope):\n",
    "            #Input and visual encoding layers\n",
    "            self.scalar_input = tf.placeholder(tf.float32, [None, 17 * 17 * 1])\n",
    "            self.features_input = tf.placeholder(tf.float32, [None, 20])\n",
    "\n",
    "            xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            self.img_in = tf.reshape(self.scalar_input, [-1, 17, 17, 1])\n",
    "            self.conv1 = tf.layers.conv2d(self.img_in, 32, 5, 2, activation=tf.nn.elu,\n",
    "                                          kernel_initializer=xavier_init)\n",
    "            self.conv2 = tf.layers.conv2d(self.conv1, 32, 3, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "            self.conv3 = tf.layers.conv2d(self.conv2, self.h_size, 5, activation=tf.nn.elu,\n",
    "                                          kernel_initializer=xavier_init)\n",
    "            self.conv3_flat = tf.contrib.layers.flatten(self.conv3)\n",
    "#             self.conv3_flat = tf.layers.dropout(self.conv3_flat)\n",
    "\n",
    "            self.d1 = tf.layers.dense(self.features_input, 32, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "#             self.d1 = tf.layers.dropout(self.d1)\n",
    "            self.d2 = tf.layers.dense(self.d1, self.h_size, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "#             self.d2 = tf.layers.dropout(self.d2)\n",
    "\n",
    "            self.merge = tf.concat([self.conv3_flat, self.d2], axis=1)\n",
    "            self.d3 = tf.layers.dense(self.merge, self.h_size, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "#             self.d3 = tf.layers.dropout(self.d3)\n",
    "            self.d4 = tf.layers.dense(self.d3, self.h_size, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "\n",
    "            #Output layers for policy and value estimations\n",
    "            self.policy = tf.layers.dense(self.d4, a_size,\n",
    "                activation=tf.nn.softmax,\n",
    "                kernel_initializer=normalized_columns_initializer(0.01))\n",
    "            self.predict = tf.argmax(self.policy, 1)\n",
    "            self.value = tf.layers.dense(self.d4, 1,\n",
    "                activation=None,\n",
    "                kernel_initializer=normalized_columns_initializer(1.0))\n",
    "\n",
    "            #Only the worker network need ops for loss functions and gradient updating.\n",
    "            if scope != 'global' and is_train:\n",
    "                self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "                self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "                self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "\n",
    "                self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "                #Loss functions\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "                self.entropy = tf.reduce_sum(-self.policy * tf.log(tf.maximum(self.policy, 0.00001))\\\n",
    "                                             - (1 - self.policy)\\\n",
    "                                             * tf.log(1 - tf.maximum(0.00001, self.policy)))\n",
    "                self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs)*self.advantages)\n",
    "                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.05\n",
    "\n",
    "                #Get gradients from local network using local losses\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss,local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients, 200.0)\n",
    "\n",
    "                #Apply local gradients to global network\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class A3CPlayer(BasePokerPlayer):\n",
    "    '''\n",
    "    A3C Player, bot wich use A3C architecture.\n",
    "\n",
    "    Parametrs\n",
    "    ---------\n",
    "    h_size : shape of layer after conv part (also before double part too)\n",
    "    \n",
    "    is_debug  wheter or not to print the debug information\n",
    "    '''\n",
    "    def __init__(self, a_size, h_size=64, debug=False):\n",
    "        self.h_size = h_size\n",
    "        self.debug = debug\n",
    "        \n",
    "        with open('../cache/hole_card_estimation.pkl', 'rb') as f:\n",
    "            self.hole_card_est = pickle.load(f)\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        self.net = AC_Network(a_size, 'worker_0', tf.train.AdamOptimizer(), is_train=False)\n",
    "        self.saver = tf.train.Saver()\n",
    "            \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "\n",
    "    def _print(self, *msg):\n",
    "        if self.debug:\n",
    "            print(msg)\n",
    "        \n",
    "    def declare_action(self, valid_actions, hole_card, round_state):\n",
    "        street = round_state['street']\n",
    "        bank = round_state['pot']['main']['amount']\n",
    "        stack = [s['stack'] for s in round_state['seats'] if s['uuid'] == self.uuid][0]\n",
    "        other_stacks = [s['stack'] for s in round_state['seats'] if s['uuid'] != self.uuid]\n",
    "        dealer_btn = round_state['dealer_btn']\n",
    "        small_blind_pos = round_state['small_blind_pos']\n",
    "        big_blind_pos = round_state['big_blind_pos']\n",
    "        next_player = round_state['next_player']\n",
    "        round_count = round_state['round_count']\n",
    "        estimation = self.hole_card_est[(hole_card[0], hole_card[1])]\n",
    "\n",
    "        \n",
    "        self.features = get_street(street)\n",
    "        self.features.extend([bank, stack, dealer_btn, small_blind_pos, big_blind_pos, next_player, round_count])\n",
    "        self.features.extend(other_stacks)\n",
    "        self.features.append(estimation)\n",
    "        \n",
    "        img_state = img_from_state(hole_card, round_state)\n",
    "        img_state = process_img(img_state)\n",
    "        \n",
    "        action_num, policy, value = self.sess.run([self.net.predict, self.net.policy, self.net.value],\n",
    "                                   feed_dict={self.net.scalar_input: [img_state],\n",
    "                                              self.net.features_input: [self.features]})\n",
    "        action_num, policy, value = action_num[0], policy[0], value[0]\n",
    "        self._print('Policy:', policy)\n",
    "        self._print('Value:', value)\n",
    "        action, amount = get_action_by_num(action_num, valid_actions)                    \n",
    "        \n",
    "        return action, amount\n",
    "        \n",
    "    def receive_game_start_message(self, game_info):\n",
    "        pass\n",
    "    \n",
    "    def receive_round_start_message(self, round_count, hole_card, seats):\n",
    "        self._print(['Hole:', hole_card])        \n",
    "        self.start_stack = [s['stack'] for s in seats if s['uuid'] == self.uuid][0]\n",
    "        self._print(['Start stack:', self.start_stack])\n",
    "        estimation = self.hole_card_est[(hole_card[0], hole_card[1])]\n",
    "        self._print(['Estimation:', estimation])\n",
    "    \n",
    "    def receive_street_start_message(self, street, round_state):\n",
    "        pass\n",
    "            \n",
    "    def receive_game_update_message(self, action, round_state):\n",
    "        pass\n",
    "    \n",
    "    def receive_round_result_message(self, winners, hand_info, round_state):\n",
    "        end_stack = [s['stack'] for s in round_state['seats'] if s['uuid'] == self.uuid][0]\n",
    "        self._print(['End stack:', end_stack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self, name, a_size, trainer, model_path, global_episodes):    \n",
    "        with open('../cache/hole_card_estimation.pkl', 'rb') as f:\n",
    "            self.hole_card_est = pickle.load(f)\n",
    "            \n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name        \n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.summary.FileWriter(\"../log/A3C/train_\"+str(self.number))\n",
    "\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_AC = AC_Network(a_size,self.name,trainer)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)    \n",
    "        \n",
    "        self.emul = MyEmulator()\n",
    "        self.my_uuid = '9'\n",
    "    \n",
    "        self.actions = self.actions = np.identity(a_size,dtype=bool).tolist()\n",
    "        \n",
    "    def init_emul(self, my_uuid='9'):\n",
    "        emul = MyEmulator()\n",
    "        emul.set_game_rule(9, 50, 15, 0)\n",
    "        self.my_uuid = my_uuid\n",
    "        self.players_info = {\n",
    "            \"1\": { \"name\": \"f1\", \"stack\": 1500 },\n",
    "            \"2\": { \"name\": \"f2\", \"stack\": 1500 },\n",
    "            \"3\": { \"name\": \"f3\", \"stack\": 1500 },\n",
    "            \"4\": { \"name\": \"f4\", \"stack\": 1500 },\n",
    "            \"5\": { \"name\": \"f5\", \"stack\": 1500 },\n",
    "            \"6\": { \"name\": \"f6\", \"stack\": 1500 },\n",
    "            \"7\": { \"name\": \"f7\", \"stack\": 1500 },\n",
    "            \"8\": { \"name\": \"f8\", \"stack\": 1500 },\n",
    "            \"9\": { \"name\": \"f9\", \"stack\": 1500 }\n",
    "        }\n",
    "    \n",
    "        emul.register_player(\"1\", pm.CallPlayer())\n",
    "        emul.register_player(\"2\", pm.CallPlayer())\n",
    "        emul.register_player(\"3\", pm.FoldPlayer())\n",
    "        emul.register_player(\"4\", pm.FoldPlayer())\n",
    "        emul.register_player(\"5\", pm.HeuristicPlayer())\n",
    "        emul.register_player(\"6\", pm.HeuristicPlayer())\n",
    "        emul.register_player(\"7\", pm.RandomPlayer())\n",
    "        emul.register_player(\"8\", pm.RandomPlayer())\n",
    "        emul.register_player(\"9\", pm.CallPlayer())\n",
    "        self.emul = emul\n",
    "        \n",
    "    def train(self,rollout,sess,gamma,bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        last_img_states = rollout[:,0]\n",
    "        last_features = rollout[:,1]\n",
    "        last_actions_num = rollout[:,2]\n",
    "        rewards = rollout[:,3]\n",
    "        img_states = rollout[:,4]\n",
    "        features = rollout[:,5]\n",
    "        values = rollout[:,7]\n",
    "\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        discounted_rewards = rewards\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        feed_dict = {self.local_AC.target_v:discounted_rewards,\n",
    "            self.local_AC.scalar_input:np.vstack(last_img_states),\n",
    "            self.local_AC.features_input:np.vstack(last_features),\n",
    "            self.local_AC.actions:last_actions_num,\n",
    "            self.local_AC.advantages:advantages}\n",
    "\n",
    "        v_l,p_l,e_l,g_n,v_n, _ = sess.run([self.local_AC.value_loss,\n",
    "            self.local_AC.policy_loss,\n",
    "            self.local_AC.entropy,\n",
    "            self.local_AC.grad_norms,\n",
    "            self.local_AC.var_norms,\n",
    "            self.local_AC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout),e_l / len(rollout), g_n, v_n\n",
    "        \n",
    "    def work(self,gamma,sess,coord,saver):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        total_steps = 0\n",
    "        print (\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():                 \n",
    "            while not coord.should_stop():\n",
    "                self.init_emul(str(np.random.randint(1, 10)))\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                d = False\n",
    "                \n",
    "                initial_state = self.emul.generate_initial_game_state(self.players_info)\n",
    "                msgs = []\n",
    "                game_state, events = self.emul.start_new_round(initial_state)\n",
    "                is_last_round = False\n",
    "\n",
    "                last_img_state = None\n",
    "                last_features = None\n",
    "                last_action_num = None\n",
    "                last_v = None\n",
    "\n",
    "                round_buffer = []\n",
    "                while not is_last_round:\n",
    "                    #Take an action using probabilities from policy network output.\n",
    "                    a = self.emul.run_until_my_next_action(game_state, self.my_uuid, msgs)\n",
    "                    \n",
    "                    if len(a) == 4:\n",
    "                        game_state, valid_actions, hole_card, round_state = a\n",
    "                        img_state = img_from_state(hole_card, round_state)\n",
    "                        img_state = process_img(img_state)\n",
    "\n",
    "                        street = round_state['street']\n",
    "                        bank = round_state['pot']['main']['amount']\n",
    "                        stack = [s['stack'] for s in round_state['seats'] if s['uuid'] == self.my_uuid][0]\n",
    "                        other_stacks = [s['stack'] for s in round_state['seats'] if s['uuid'] != self.my_uuid]\n",
    "                        dealer_btn = round_state['dealer_btn']\n",
    "                        small_blind_pos = round_state['small_blind_pos']\n",
    "                        big_blind_pos = round_state['big_blind_pos']\n",
    "                        next_player = round_state['next_player']\n",
    "                        round_count = round_state['round_count']\n",
    "                        estimation = self.hole_card_est[(hole_card[0], hole_card[1])]\n",
    "\n",
    "                        features = get_street(street)\n",
    "                        features.extend([bank, stack, dealer_btn, small_blind_pos, big_blind_pos, next_player,\n",
    "                                         round_count])\n",
    "                        features.extend(other_stacks)\n",
    "                        features.append(estimation)\n",
    "                     \n",
    "                        # add to buffer last hand \n",
    "                        if last_img_state is not None:\n",
    "                            round_buffer.append([last_img_state, last_features, last_action_num, 0, img_state,\n",
    "                                                   features, 0, last_v[0, 0]])\n",
    "                            episode_values.append(last_v[0, 0])\n",
    "                     \n",
    "                        pol_val = sess.run([self.local_AC.policy, self.local_AC.value],\n",
    "                                              feed_dict={self.local_AC.scalar_input: [img_state],\n",
    "                                                         self.local_AC.features_input: [features]})\n",
    "                        a_dist, v = pol_val[0], pol_val[1]\n",
    "\n",
    "                        a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                        a = np.argmax(a_dist == a)\n",
    "                        action, amount = get_action_by_num(a, valid_actions)\n",
    "                        game_state, msgs = self.emul.apply_my_action(game_state, action, amount)\n",
    "\n",
    "                        last_img_state = img_state.copy()\n",
    "                        last_features = features.copy()\n",
    "                        last_action_num = a\n",
    "                        last_v = v\n",
    "                    else: # round end\n",
    "                        game_state, reward = a\n",
    "                        reward /= 100\n",
    "                        episode_reward += reward\n",
    "\n",
    "                        if reward >= 0:\n",
    "                            reward = np.log(1 + reward)\n",
    "                        else:\n",
    "                            reward = -np.log(1 - reward)\n",
    "\n",
    "                        # add to buffer last hand \n",
    "                        if last_img_state is not None:\n",
    "                            round_buffer.append([last_img_state, last_features, last_action_num, reward,\n",
    "                                                   last_img_state, last_features, 1, last_v[0, 0]])\n",
    "                            episode_values.append(last_v[0,0])\n",
    "\n",
    "                            # apply same reward for all states in round\n",
    "                            for k in range(len(round_buffer)):\n",
    "                                round_buffer[k][3] = reward\n",
    "                                \n",
    "                        episode_buffer.extend(round_buffer)\n",
    "                        round_buffer = []\n",
    "\n",
    "                        is_last_round = self.emul._is_last_round(game_state, self.emul.game_rule)\n",
    "                        game_state, events = self.emul.start_new_round(game_state)\n",
    "\n",
    "                        last_img_state = None\n",
    "                        last_action_num = None   \n",
    "                        last_v = None\n",
    "                        \n",
    "                    self.episode_buffer = episode_buffer # for debug\n",
    "                    self.episode_values = episode_values\n",
    "\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "                                            \n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "                \n",
    "                # Update the network using the episode buffer at the end of the episode.\n",
    "                if len(episode_buffer) != 0:\n",
    "                    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
    "                                \n",
    "                    \n",
    "                if episode_count % 5 == 0 and self.name == 'worker_0':\n",
    "                    saver.save(sess, self.model_path, episode_count)\n",
    "                    print (\"Saved Model\", episode_count)\n",
    "                     \n",
    "                if episode_count % 1 == 0:\n",
    "                    mean_reward = np.mean(self.episode_rewards[-3:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-3:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-3:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Perf/Reward', simple_value=float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Length', simple_value=float(mean_length))\n",
    "                    summary.value.add(tag='Perf/Value', simple_value=float(mean_value))\n",
    "                    summary.value.add(tag='Losses/Value Loss', simple_value=float(v_l))\n",
    "                    summary.value.add(tag='Losses/Policy Loss', simple_value=float(p_l))\n",
    "                    summary.value.add(tag='Losses/Entropy', simple_value=float(e_l))\n",
    "                    summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                    summary.value.add(tag='Losses/Var Norm', simple_value=float(v_n))\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                \n",
    "                # for debug!\n",
    "                if episode_count == 10 and self.name == 'worker_0':\n",
    "                    coord.request_stop()\n",
    "                \n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = .99 # discount rate for advantage estimation and reward discounting\n",
    "a_size = 5 # Agent can move Left, Right, or Fire\n",
    "load_model = False\n",
    "model_path = '../cache/models/A3C/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting worker 0\n",
      "Starting worker 1\n",
      "Starting worker 2\n",
      "Starting worker 3\n",
      "Saved Model 0\n",
      "Saved Model 5\n",
      "Saved Model 10\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    master_network = AC_Network(a_size,'global',None) # Generate global network\n",
    "    num_workers = multiprocessing.cpu_count() # Set workers ot number of available CPU threads\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(i,a_size,trainer,model_path,global_episodes))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    # This is where the asynchronous magic happens.\n",
    "    # Start the \"work\" process for each worker in a separate threat.\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.work(gamma,sess,coord,saver)\n",
    "        t = threading.Thread(target=(worker_work))\n",
    "        t.start()\n",
    "        sleep(0.5)\n",
    "        worker_threads.append(t)\n",
    "    coord.join(worker_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir=worker_0:'./train_0',worker_1:'./train_1',worker_2:'./train_2',worker_3:'./train_3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../cache/models/A3C/-10\n"
     ]
    }
   ],
   "source": [
    "player = A3CPlayer(5, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Hole:', ['S5', 'H2']],)\n",
      "(['Start stack:', 1500],)\n",
      "(['Estimation:', 0.0552],)\n",
      "Started the round 1\n",
      "Street \"preflop\" started. (community card = [])\n",
      "\"FoldPlayer1\" declared \"fold:0\"\n",
      "\"FoldPlayer2\" declared \"fold:0\"\n",
      "\"HeuristicPlayer1\" declared \"fold:0\"\n",
      "\"HeuristicPlayer2\" declared \"fold:0\"\n",
      "\"RandomPlayer1\" declared \"fold:0\"\n",
      "\"RandomPlayer2\" declared \"raise:237\"\n",
      "('Policy:', array([  2.83780333e-04,   8.59752981e-05,   1.40973998e-06,\n",
      "         3.65669757e-01,   6.33959115e-01], dtype=float32))\n",
      "('Value:', array([ 44.74306488], dtype=float32))\n",
      "\"A3C\" declared \"raise:750\"\n",
      "\"CallPlayer1\" declared \"call:750\"\n",
      "\"CallPlayer2\" declared \"call:750\"\n",
      "\"RandomPlayer2\" declared \"fold:0\"\n",
      "Street \"flop\" started. (community card = ['S2', 'DJ', 'S6'])\n",
      "\"CallPlayer1\" declared \"call:0\"\n",
      "\"CallPlayer2\" declared \"call:0\"\n",
      "('Policy:', array([  1.04031397e-03,   3.19803803e-04,   1.40738885e-05,\n",
      "         8.68916154e-01,   1.29709676e-01], dtype=float32))\n",
      "('Value:', array([-1.25970757], dtype=float32))\n",
      "\"A3C\" declared \"raise:750\"\n",
      "\"CallPlayer1\" declared \"call:750\"\n",
      "\"CallPlayer2\" declared \"call:750\"\n",
      "Street \"turn\" started. (community card = ['S2', 'DJ', 'S6', 'HA'])\n",
      "Street \"river\" started. (community card = ['S2', 'DJ', 'S6', 'HA', 'DQ'])\n",
      "\"['CallPlayer2']\" won the round 1 (stack = {'A3C': 0, 'CallPlayer1': 0, 'CallPlayer2': 4737, 'FoldPlayer1': 1500, 'FoldPlayer2': 1500, 'HeuristicPlayer1': 1500, 'HeuristicPlayer2': 1500, 'RandomPlayer1': 1500, 'RandomPlayer2': 1263})\n",
      "(['End stack:', 0],)\n",
      "(['Hole:', ['H3', 'CK']],)\n",
      "(['Start stack:', 0],)\n",
      "(['Estimation:', 0.0909],)\n",
      "Started the round 2\n",
      "Street \"preflop\" started. (community card = [])\n",
      "\"HeuristicPlayer1\" declared \"fold:0\"\n",
      "\"HeuristicPlayer2\" declared \"fold:0\"\n",
      "\"RandomPlayer1\" declared \"raise:1146\"\n",
      "\"RandomPlayer2\" declared \"raise:1263\"\n",
      "\"CallPlayer2\" declared \"call:1263\"\n",
      "\"FoldPlayer1\" declared \"fold:0\"\n",
      "\"FoldPlayer2\" declared \"fold:0\"\n",
      "\"RandomPlayer1\" declared \"fold:0\"\n",
      "Street \"flop\" started. (community card = ['S8', 'D7', 'DQ'])\n",
      "Street \"turn\" started. (community card = ['S8', 'D7', 'DQ', 'HJ'])\n",
      "Street \"river\" started. (community card = ['S8', 'D7', 'DQ', 'HJ', 'CA'])\n",
      "\"['RandomPlayer2']\" won the round 2 (stack = {'A3C': 0, 'CallPlayer1': 0, 'CallPlayer2': 3474, 'FoldPlayer1': 1485, 'FoldPlayer2': 1470, 'HeuristicPlayer1': 1500, 'HeuristicPlayer2': 1500, 'RandomPlayer1': 354, 'RandomPlayer2': 3717})\n",
      "(['End stack:', 0],)\n"
     ]
    }
   ],
   "source": [
    "config = setup_config(max_round=2, initial_stack=1500, small_blind_amount=15, summary_file='/dev/null')\n",
    "\n",
    "config.register_player(name=\"A3C\", algorithm=player)\n",
    "config.register_player(name=\"CallPlayer1\", algorithm=pm.CallPlayer())\n",
    "config.register_player(name=\"CallPlayer2\", algorithm=pm.CallPlayer())\n",
    "config.register_player(name=\"FoldPlayer1\", algorithm=pm.FoldPlayer())\n",
    "config.register_player(name=\"FoldPlayer2\", algorithm=pm.FoldPlayer())\n",
    "config.register_player(name=\"HeuristicPlayer1\", algorithm=pm.HeuristicPlayer())\n",
    "config.register_player(name=\"HeuristicPlayer2\", algorithm=pm.HeuristicPlayer())\n",
    "config.register_player(name=\"RandomPlayer1\", algorithm=pm.RandomPlayer())\n",
    "config.register_player(name=\"RandomPlayer2\", algorithm=pm.RandomPlayer())\n",
    "\n",
    "game_result = start_poker(config, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../cache/models/A3C/-10\n"
     ]
    }
   ],
   "source": [
    "player = A3CPlayer(5, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = setup_config(max_round=50, initial_stack=1500, small_blind_amount=15, summary_file='/dev/null')\n",
    "\n",
    "config.register_player(name=\"A3C\", algorithm=player)\n",
    "config.register_player(name=\"CallPlayer1\", algorithm=pm.CallPlayer())\n",
    "config.register_player(name=\"CallPlayer2\", algorithm=pm.CallPlayer())\n",
    "config.register_player(name=\"FoldPlayer1\", algorithm=pm.FoldPlayer())\n",
    "config.register_player(name=\"FoldPlayer2\", algorithm=pm.FoldPlayer())\n",
    "config.register_player(name=\"HeuristicPlayer1\", algorithm=pm.HeuristicPlayer())\n",
    "config.register_player(name=\"HeuristicPlayer2\", algorithm=pm.HeuristicPlayer())\n",
    "config.register_player(name=\"RandomPlayer1\", algorithm=pm.RandomPlayer())\n",
    "config.register_player(name=\"RandomPlayer2\", algorithm=pm.RandomPlayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.4 s, sys: 0 ns, total: 28.4 s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d = None\n",
    "for i in range(2): # 100\n",
    "    game_result = start_poker(config, verbose=0)\n",
    "    t = pd.DataFrame(game_result['players'])\n",
    "    t['round'] = i\n",
    "    if d is None:\n",
    "        d = t\n",
    "    else:\n",
    "        d = pd.concat((d, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "A3C                    0.0\n",
       "RandomPlayer1          0.0\n",
       "RandomPlayer2          0.0\n",
       "HeuristicPlayer1    1012.5\n",
       "HeuristicPlayer2    1027.5\n",
       "FoldPlayer2         1050.0\n",
       "FoldPlayer1         1080.0\n",
       "CallPlayer2         4650.0\n",
       "CallPlayer1         4680.0\n",
       "Name: stack, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.groupby('name').mean()['stack'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
