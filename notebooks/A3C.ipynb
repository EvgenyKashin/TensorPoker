{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pypokerengine.players import BasePokerPlayer\n",
    "from pypokerengine.utils.card_utils import Card, Deck\n",
    "from pypokerengine.api.game import setup_config, start_poker\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import scipy\n",
    "import scipy.signal\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../scripts/')\n",
    "\n",
    "import PlayerModels as pm\n",
    "from MyEmulator import MyEmulator\n",
    "# from DQNPlayer import DQNPlayer\n",
    "from util import *\n",
    "\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "from random import choice\n",
    "from time import sleep\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "# Discounting function used to calculate discounted returns.\n",
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "#Used to initialize weights for policy and value output layers\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self, a_size, scope, trainer, h_size=64, is_train=True):\n",
    "        self.h_size = h_size\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        with tf.variable_scope(scope):\n",
    "            #Input and visual encoding layers\n",
    "            self.scalar_input = tf.placeholder(tf.float32, [None, 17 * 17 * 1])\n",
    "            self.features_input = tf.placeholder(tf.float32, [None, 20])\n",
    "\n",
    "            xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            self.img_in = tf.reshape(self.scalar_input, [-1, 17, 17, 1])\n",
    "            self.conv1 = tf.layers.conv2d(self.img_in, 32, 5, 2, activation=tf.nn.elu,\n",
    "                                          kernel_initializer=xavier_init)\n",
    "            self.conv2 = tf.layers.conv2d(self.conv1, 32, 3, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "            self.conv3 = tf.layers.conv2d(self.conv2, self.h_size, 5, activation=tf.nn.elu,\n",
    "                                          kernel_initializer=xavier_init)\n",
    "            self.conv3_flat = tf.contrib.layers.flatten(self.conv3)\n",
    "    #             self.conv3_flat = tf.layers.dropout(self.conv3_flat)\n",
    "\n",
    "            self.d1 = tf.layers.dense(self.features_input, 32, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "    #             self.d1 = tf.layers.dropout(self.d1)\n",
    "            self.d2 = tf.layers.dense(self.d1, self.h_size, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "    #             self.d2 = tf.layers.dropout(self.d2)\n",
    "\n",
    "            self.merge = tf.concat([self.conv3_flat, self.d2], axis=1)\n",
    "            self.d3 = tf.layers.dense(self.merge, self.h_size, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "    #             self.d3 = tf.layers.dropout(self.d3)\n",
    "            self.d4 = tf.layers.dense(self.d3, self.h_size, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "\n",
    "            #Output layers for policy and value estimations\n",
    "            self.policy = tf.layers.dense(self.d4, a_size,\n",
    "                activation=tf.nn.softmax,\n",
    "                kernel_initializer=normalized_columns_initializer(0.01))\n",
    "            self.predict = tf.argmax(self.policy, 1)\n",
    "            self.value = tf.layers.dense(self.d4, 1,\n",
    "                activation=None,\n",
    "                kernel_initializer=normalized_columns_initializer(1.0))\n",
    "\n",
    "            #Only the worker network need ops for loss functions and gradient updating.\n",
    "            if scope != 'global' and is_train:\n",
    "                self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "                self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "                self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "\n",
    "                self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "                #Loss functions\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "                self.entropy = tf.reduce_sum(-self.policy * tf.log(tf.maximum(self.policy, 0.00001))\\\n",
    "                                             - (1 - self.policy)\\\n",
    "                                             * tf.log(1 - tf.maximum(0.00001, self.policy)))\n",
    "                self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs)*self.advantages)\n",
    "                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.05\n",
    "\n",
    "                #Get gradients from local network using local losses\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss,local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients, 200.0)\n",
    "    #                 self.grad_norms = tf.global_norm(grads)\n",
    "\n",
    "                #Apply local gradients to global network\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class A3CPlayer(BasePokerPlayer):\n",
    "    '''\n",
    "    A3C Player, bot wich use A3C architecture.\n",
    "\n",
    "    Parametrs\n",
    "    ---------\n",
    "    h_size : shape of layer after conv part (also before double part too)\n",
    "    '''\n",
    "    def __init__(self, a_size, h_size=64, debug=False):\n",
    "        self.h_size = h_size\n",
    "        self.debug = debug\n",
    "        \n",
    "        with open('../cache/hole_card_estimation.pkl', 'rb') as f:\n",
    "            self.hole_card_est = pickle.load(f)\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        self.net = AC_Network(a_size, 'train_0', tf.train.AdamOptimizer(), is_train=False)\n",
    "        self.saver = tf.train.Saver()\n",
    "            \n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state('../cache/models/A3C/')\n",
    "        self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "\n",
    "    def _print(self, *msg):\n",
    "        if self.debug:\n",
    "            print(msg)\n",
    "        \n",
    "    def declare_action(self, valid_actions, hole_card, round_state):\n",
    "        street = round_state['street']\n",
    "        bank = round_state['pot']['main']['amount']\n",
    "        stack = [s['stack'] for s in round_state['seats'] if s['uuid'] == self.uuid][0]\n",
    "        other_stacks = [s['stack'] for s in round_state['seats'] if s['uuid'] != self.uuid]\n",
    "        dealer_btn = round_state['dealer_btn']\n",
    "        small_blind_pos = round_state['small_blind_pos']\n",
    "        big_blind_pos = round_state['big_blind_pos']\n",
    "        next_player = round_state['next_player']\n",
    "        round_count = round_state['round_count']\n",
    "        estimation = self.hole_card_est[(hole_card[0], hole_card[1])]\n",
    "\n",
    "        \n",
    "        self.features = get_street(street)\n",
    "        self.features.extend([bank, stack, dealer_btn, small_blind_pos, big_blind_pos, next_player, round_count])\n",
    "        self.features.extend(other_stacks)\n",
    "        self.features.append(estimation)\n",
    "        \n",
    "        img_state = img_from_state(hole_card, round_state)\n",
    "        img_state = process_img(img_state)\n",
    "        \n",
    "        action_num = self.sess.run(self.net.predict, feed_dict={self.scalar_input: [img_state],\n",
    "                                                                self.features_input: [self.features]})[0]\n",
    "        self._print(qs)\n",
    "        action, amount = get_action_by_num(action_num, valid_actions)                    \n",
    "        \n",
    "        return action, amount\n",
    "        \n",
    "    def receive_game_start_message(self, game_info):\n",
    "        pass\n",
    "    \n",
    "    def receive_round_start_message(self, round_count, hole_card, seats):\n",
    "        self._print(['Hole:', hole_card])        \n",
    "        self.start_stack = [s['stack'] for s in seats if s['uuid'] == self.uuid][0]\n",
    "        self._print(['Start stack:', self.start_stack])\n",
    "        estimation = self.hole_card_est[(hole_card[0], hole_card[1])]\n",
    "        self._print(['Estimation:', estimation])\n",
    "    \n",
    "    def receive_street_start_message(self, street, round_state):\n",
    "        pass\n",
    "            \n",
    "    def receive_game_update_message(self, action, round_state):\n",
    "        pass\n",
    "    \n",
    "    def receive_round_result_message(self, winners, hand_info, round_state):\n",
    "        end_stack = [s['stack'] for s in round_state['seats'] if s['uuid'] == self.uuid][0]\n",
    "        self._print(['End stack:', end_stack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_emul(my_uuid_):\n",
    "    global my_uuid\n",
    "    my_uuid = my_uuid_\n",
    "\n",
    "    emul.register_player(\"1\", pm.CallPlayer())\n",
    "    emul.register_player(\"2\", pm.CallPlayer())\n",
    "    emul.register_player(\"3\", pm.FoldPlayer())\n",
    "    emul.register_player(\"4\", pm.FoldPlayer())\n",
    "    emul.register_player(\"5\", pm.HeuristicPlayer())\n",
    "    emul.register_player(\"6\", pm.HeuristicPlayer())\n",
    "    emul.register_player(\"7\", pm.RandomPlayer())\n",
    "    emul.register_player(\"8\", pm.RandomPlayer())\n",
    "    emul.register_player(\"9\", pm.CallPlayer())\n",
    "\n",
    "\n",
    "    players_info = {\n",
    "        \"1\": { \"name\": \"CallPlayer1\", \"stack\": 1500 },\n",
    "        \"2\": { \"name\": \"CallPlayer2\", \"stack\": 1500 },\n",
    "        \"3\": { \"name\": \"FoldPlayer1\", \"stack\": 1500 },\n",
    "        \"4\": { \"name\": \"FoldPlayer2\", \"stack\": 1500 },\n",
    "        \"5\": { \"name\": \"HeuristicPlayer1\", \"stack\": 1500 },\n",
    "        \"6\": { \"name\": \"HeuristicPlayer2\", \"stack\": 1500 },\n",
    "        \"7\": { \"name\": \"RandomPlayer1\", \"stack\": 1500 },\n",
    "        \"8\": { \"name\": \"RandomPlayer2\", \"stack\": 1500 },\n",
    "        \"9\": { \"name\": \"DQN\", \"stack\": 1500 }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self, name, a_size, trainer, model_path, global_episodes):    \n",
    "        with open('../cache/hole_card_estimation.pkl', 'rb') as f:\n",
    "            self.hole_card_est = pickle.load(f)\n",
    "            \n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name        \n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.summary.FileWriter(\"../log/A3C/train_\"+str(self.number))\n",
    "\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_AC = AC_Network(a_size,self.name,trainer)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)    \n",
    "        \n",
    "        emul = MyEmulator()\n",
    "        emul.set_game_rule(9, 50, 15, 0)\n",
    "        self.my_uuid = '9'\n",
    "        self.players_info = {\n",
    "            \"1\": { \"name\": \"f1\", \"stack\": 1500 },\n",
    "            \"2\": { \"name\": \"f2\", \"stack\": 1500 },\n",
    "            \"3\": { \"name\": \"f3\", \"stack\": 1500 },\n",
    "            \"4\": { \"name\": \"f4\", \"stack\": 1500 },\n",
    "            \"5\": { \"name\": \"f5\", \"stack\": 1500 },\n",
    "            \"6\": { \"name\": \"f6\", \"stack\": 1500 },\n",
    "            \"7\": { \"name\": \"f7\", \"stack\": 1500 },\n",
    "            \"8\": { \"name\": \"f8\", \"stack\": 1500 },\n",
    "            \"9\": { \"name\": \"f9\", \"stack\": 1500 }\n",
    "        }\n",
    "    \n",
    "        emul.register_player(\"1\", pm.CallPlayer())\n",
    "        emul.register_player(\"2\", pm.CallPlayer())\n",
    "        emul.register_player(\"3\", pm.FoldPlayer())\n",
    "        emul.register_player(\"4\", pm.FoldPlayer())\n",
    "        emul.register_player(\"5\", pm.HeuristicPlayer())\n",
    "        emul.register_player(\"6\", pm.HeuristicPlayer())\n",
    "        emul.register_player(\"7\", pm.RandomPlayer())\n",
    "        emul.register_player(\"8\", pm.RandomPlayer())\n",
    "        emul.register_player(\"9\", pm.CallPlayer())\n",
    "    \n",
    "        self.actions = self.actions = np.identity(a_size,dtype=bool).tolist()\n",
    "        self.enul = emul\n",
    "        \n",
    "    def init_emul(self, my_uuid='9'):\n",
    "        emul = MyEmulator()\n",
    "        emul.set_game_rule(9, 50, 15, 0)\n",
    "        self.my_uuid = my_uuid\n",
    "        self.players_info = {\n",
    "            \"1\": { \"name\": \"f1\", \"stack\": 1500 },\n",
    "            \"2\": { \"name\": \"f2\", \"stack\": 1500 },\n",
    "            \"3\": { \"name\": \"f3\", \"stack\": 1500 },\n",
    "            \"4\": { \"name\": \"f4\", \"stack\": 1500 },\n",
    "            \"5\": { \"name\": \"f5\", \"stack\": 1500 },\n",
    "            \"6\": { \"name\": \"f6\", \"stack\": 1500 },\n",
    "            \"7\": { \"name\": \"f7\", \"stack\": 1500 },\n",
    "            \"8\": { \"name\": \"f8\", \"stack\": 1500 },\n",
    "            \"9\": { \"name\": \"f9\", \"stack\": 1500 }\n",
    "        }\n",
    "    \n",
    "        emul.register_player(\"1\", pm.CallPlayer())\n",
    "        emul.register_player(\"2\", pm.CallPlayer())\n",
    "        emul.register_player(\"3\", pm.FoldPlayer())\n",
    "        emul.register_player(\"4\", pm.FoldPlayer())\n",
    "        emul.register_player(\"5\", pm.HeuristicPlayer())\n",
    "        emul.register_player(\"6\", pm.HeuristicPlayer())\n",
    "        emul.register_player(\"7\", pm.RandomPlayer())\n",
    "        emul.register_player(\"8\", pm.RandomPlayer())\n",
    "        emul.register_player(\"9\", pm.CallPlayer())\n",
    "        self.emul = emul\n",
    "        \n",
    "    def train(self,rollout,sess,gamma,bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        last_img_states = rollout[:,0]\n",
    "        last_features = rollout[:,1]\n",
    "        last_actions_num = rollout[:,2]\n",
    "        rewards = rollout[:,3]\n",
    "        img_states = rollout[:,4]\n",
    "        features = rollout[:,5]\n",
    "        values = rollout[:,7]\n",
    "\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        discounted_rewards = rewards\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        feed_dict = {self.local_AC.target_v:discounted_rewards,\n",
    "            self.local_AC.scalar_input:np.vstack(last_img_states),\n",
    "            self.local_AC.features_input:np.vstack(last_features),\n",
    "            self.local_AC.actions:last_actions_num,\n",
    "            self.local_AC.advantages:advantages}\n",
    "\n",
    "        v_l,p_l,e_l,g_n,v_n, _ = sess.run([self.local_AC.value_loss,\n",
    "            self.local_AC.policy_loss,\n",
    "            self.local_AC.entropy,\n",
    "            self.local_AC.grad_norms,\n",
    "            self.local_AC.var_norms,\n",
    "            self.local_AC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout),e_l / len(rollout), g_n, v_n\n",
    "        \n",
    "    def work(self,gamma,sess,coord,saver):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        total_steps = 0\n",
    "        print (\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():                 \n",
    "            while not coord.should_stop():\n",
    "                self.init_emul(str(np.random.randint(1, 10)))\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                d = False\n",
    "                \n",
    "                initial_state = self.emul.generate_initial_game_state(self.players_info)\n",
    "                msgs = []\n",
    "                game_state, events = self.emul.start_new_round(initial_state)\n",
    "                is_last_round = False\n",
    "\n",
    "                last_img_state = None\n",
    "                last_features = None\n",
    "                last_action_num = None\n",
    "                last_v = None\n",
    "\n",
    "                round_buffer = []\n",
    "                while not is_last_round:\n",
    "                    #Take an action using probabilities from policy network output.\n",
    "                    a = self.emul.run_until_my_next_action(game_state, self.my_uuid, msgs)\n",
    "                    \n",
    "                    if len(a) == 4:\n",
    "                        game_state, valid_actions, hole_card, round_state = a\n",
    "                        img_state = img_from_state(hole_card, round_state)\n",
    "                        img_state = process_img(img_state)\n",
    "\n",
    "                        street = round_state['street']\n",
    "                        bank = round_state['pot']['main']['amount']\n",
    "                        stack = [s['stack'] for s in round_state['seats'] if s['uuid'] == self.my_uuid][0]\n",
    "                        other_stacks = [s['stack'] for s in round_state['seats'] if s['uuid'] != self.my_uuid]\n",
    "                        dealer_btn = round_state['dealer_btn']\n",
    "                        small_blind_pos = round_state['small_blind_pos']\n",
    "                        big_blind_pos = round_state['big_blind_pos']\n",
    "                        next_player = round_state['next_player']\n",
    "                        round_count = round_state['round_count']\n",
    "                        estimation = self.hole_card_est[(hole_card[0], hole_card[1])]\n",
    "\n",
    "                        features = get_street(street)\n",
    "                        features.extend([bank, stack, dealer_btn, small_blind_pos, big_blind_pos, next_player,\n",
    "                                         round_count])\n",
    "                        features.extend(other_stacks)\n",
    "                        features.append(estimation)\n",
    "                     \n",
    "                        # add to buffer last hand \n",
    "                        if last_img_state is not None:\n",
    "                            round_buffer.append([last_img_state, last_features, last_action_num, 0, img_state,\n",
    "                                                   features, 0, last_v[0, 0]])\n",
    "                            episode_values.append(last_v[0, 0])\n",
    "                     \n",
    "                        pol_val = sess.run([self.local_AC.policy, self.local_AC.value],\n",
    "                                              feed_dict={self.local_AC.scalar_input: [img_state],\n",
    "                                                         self.local_AC.features_input: [features]})\n",
    "                        a_dist, v = pol_val[0], pol_val[1]\n",
    "\n",
    "                        a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                        a = np.argmax(a_dist == a)\n",
    "                        action, amount = get_action_by_num(a, valid_actions)\n",
    "                        game_state, msgs = self.emul.apply_my_action(game_state, action, amount)\n",
    "\n",
    "                        last_img_state = img_state.copy()\n",
    "                        last_features = features.copy()\n",
    "                        last_action_num = a\n",
    "                        last_v = v\n",
    "                    else: # round end\n",
    "                        game_state, reward = a\n",
    "                        reward /= 100\n",
    "                        episode_reward += reward\n",
    "\n",
    "                        if reward >= 0:\n",
    "                            reward = np.log(1 + reward)\n",
    "                        else:\n",
    "                            reward = -np.log(1 - reward)\n",
    "\n",
    "                        # add to buffer last hand \n",
    "                        if last_img_state is not None:\n",
    "                            round_buffer.append([last_img_state, last_features, last_action_num, reward,\n",
    "                                                   last_img_state, last_features, 1, last_v[0, 0]])\n",
    "                            episode_values.append(last_v[0,0])\n",
    "\n",
    "                            # apply same reward for all states in round\n",
    "                            for k in range(len(round_buffer)):\n",
    "                                round_buffer[k][3] = reward\n",
    "                                \n",
    "                        episode_buffer.extend(round_buffer)\n",
    "                        round_buffer = []\n",
    "\n",
    "                        is_last_round = self.emul._is_last_round(game_state, self.emul.game_rule)\n",
    "                        game_state, events = self.emul.start_new_round(game_state)\n",
    "\n",
    "                        last_img_state = None\n",
    "                        last_action_num = None   \n",
    "                        last_v = None\n",
    "                        \n",
    "                    self.episode_buffer = episode_buffer # for debug\n",
    "                    self.episode_values = episode_values\n",
    "\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "                                            \n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "                \n",
    "                # Update the network using the episode buffer at the end of the episode.\n",
    "                if len(episode_buffer) != 0:\n",
    "                    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
    "                                \n",
    "                    \n",
    "                if episode_count % 10 == 0 and self.name == 'worker_0':\n",
    "                    saver.save(sess, self.model_path, episode_count)\n",
    "                    print (\"Saved Model\", episode_count)\n",
    "                     \n",
    "                if episode_count % 1 == 0:\n",
    "                    mean_reward = np.mean(self.episode_rewards[-3:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-3:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-3:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Perf/Reward', simple_value=float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Length', simple_value=float(mean_length))\n",
    "                    summary.value.add(tag='Perf/Value', simple_value=float(mean_value))\n",
    "                    summary.value.add(tag='Losses/Value Loss', simple_value=float(v_l))\n",
    "                    summary.value.add(tag='Losses/Policy Loss', simple_value=float(p_l))\n",
    "                    summary.value.add(tag='Losses/Entropy', simple_value=float(e_l))\n",
    "                    summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                    summary.value.add(tag='Losses/Var Norm', simple_value=float(v_n))\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = .99 # discount rate for advantage estimation and reward discounting\n",
    "a_size = 5 # Agent can move Left, Right, or Fire\n",
    "load_model = False\n",
    "model_path = '../cache/models/A3C/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting worker 0\n",
      "Starting worker 1\n",
      "Starting worker 2\n",
      "Starting worker 3\n",
      "Saved Model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-8-19e237ff1914>\", line 30, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma,sess,coord,saver)\n",
      "  File \"<ipython-input-6-b1bc364b83a5>\", line 166, in work\n",
      "    self.local_AC.features_input: [features]})\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 778, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 914, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n",
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-8-19e237ff1914>\", line 30, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma,sess,coord,saver)\n",
      "  File \"<ipython-input-6-b1bc364b83a5>\", line 166, in work\n",
      "    self.local_AC.features_input: [features]})\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 778, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 914, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-8-19e237ff1914>\", line 30, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma,sess,coord,saver)\n",
      "  File \"<ipython-input-6-b1bc364b83a5>\", line 220, in work\n",
      "    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
      "  File \"<ipython-input-6-b1bc364b83a5>\", line 104, in train\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 778, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 914, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n",
      "Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-8-19e237ff1914>\", line 30, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma,sess,coord,saver)\n",
      "  File \"<ipython-input-6-b1bc364b83a5>\", line 220, in work\n",
      "    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
      "  File \"<ipython-input-6-b1bc364b83a5>\", line 104, in train\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 778, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 914, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-19e237ff1914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mworker_threads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;31m# Wait for all threads to stop or for request_stop() to be called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mwait_for_stop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    309\u001b[0m       \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCoordinator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtold\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0mexpired\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mregister_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    master_network = AC_Network(a_size,'global',None) # Generate global network\n",
    "    num_workers = multiprocessing.cpu_count() # Set workers ot number of available CPU threads\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(i,a_size,trainer,model_path,global_episodes))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    # This is where the asynchronous magic happens.\n",
    "    # Start the \"work\" process for each worker in a separate threat.\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.work(gamma,sess,coord,saver)\n",
    "        t = threading.Thread(target=(worker_work))\n",
    "        t.start()\n",
    "        sleep(0.5)\n",
    "        worker_threads.append(t)\n",
    "    coord.join(worker_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir=worker_0:'./train_0',worker_1:'./train_1',worker_2:'./train_2',worker_3:'./train_3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../cache/models/A3C/-0\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key train_0/conv2d_2/bias not found in checkpoint\n\t [[Node: save/RestoreV2_4 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_4/tensor_names, save/RestoreV2_4/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_4', defined at:\n  File \"/home/digitman/miniconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/digitman/miniconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-7cb00f29a204>\", line 1, in <module>\n    player = A3CPlayer(5)\n  File \"<ipython-input-4-6107ccce80d2>\", line 18, in __init__\n    self.saver = tf.train.Saver()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1056, in __init__\n    self.build()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 669, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key train_0/conv2d_2/bias not found in checkpoint\n\t [[Node: save/RestoreV2_4 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_4/tensor_names, save/RestoreV2_4/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key train_0/conv2d_2/bias not found in checkpoint\n\t [[Node: save/RestoreV2_4 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_4/tensor_names, save/RestoreV2_4/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7cb00f29a204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA3CPlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-6107ccce80d2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, a_size, h_size, debug)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../cache/models/A3C/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1457\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key train_0/conv2d_2/bias not found in checkpoint\n\t [[Node: save/RestoreV2_4 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_4/tensor_names, save/RestoreV2_4/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_4', defined at:\n  File \"/home/digitman/miniconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/digitman/miniconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-7cb00f29a204>\", line 1, in <module>\n    player = A3CPlayer(5)\n  File \"<ipython-input-4-6107ccce80d2>\", line 18, in __init__\n    self.saver = tf.train.Saver()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1056, in __init__\n    self.build()\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 669, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key train_0/conv2d_2/bias not found in checkpoint\n\t [[Node: save/RestoreV2_4 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_4/tensor_names, save/RestoreV2_4/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "player = A3CPlayer(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = setup_config(max_round=50, initial_stack=1500, small_blind_amount=15, summary_file='/dev/null')\n",
    "\n",
    "config.register_player(name=\"player\", algorithm=player)\n",
    "# config.register_player(name=\"r2\", algorithm=RandomPlayer())\n",
    "config.register_player(name=\"CallPlayer1\", algorithm=pm.CallPlayer())\n",
    "config.register_player(name=\"CallPlayer2\", algorithm=pm.CallPlayer())\n",
    "config.register_player(name=\"FoldPlayer1\", algorithm=pm.FoldPlayer())\n",
    "config.register_player(name=\"FoldPlayer2\", algorithm=pm.FoldPlayer())\n",
    "config.register_player(name=\"HeuristicPlayer1\", algorithm=pm.HeuristicPlayer())\n",
    "config.register_player(name=\"HeuristicPlayer2\", algorithm=pm.HeuristicPlayer())\n",
    "config.register_player(name=\"RandomPlayer1\", algorithm=pm.RandomPlayer())\n",
    "config.register_player(name=\"RandomPlayer2\", algorithm=pm.RandomPlayer())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
