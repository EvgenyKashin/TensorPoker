{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pypokerengine.players import BasePokerPlayer\n",
    "from pypokerengine.utils.card_utils import Card, Deck\n",
    "from pypokerengine.api.game import setup_config, start_poker\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import scipy\n",
    "import scipy.signal\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../scripts/')\n",
    "\n",
    "import PlayerModels as pm\n",
    "from MyEmulator import MyEmulator\n",
    "# from DQNPlayer import DQNPlayer\n",
    "from util import *\n",
    "\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "from random import choice\n",
    "from time import sleep\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "# Discounting function used to calculate discounted returns.\n",
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "#Used to initialize weights for policy and value output layers\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class AC_Network():\n",
    "#     def __init__(self,s_size,a_size,scope,trainer):\n",
    "#         with tf.variable_scope(scope):\n",
    "#             #Input and visual encoding layers\n",
    "#             self.inputs = tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
    "#             self.imageIn = tf.reshape(self.inputs,shape=[-1,84,84,1])\n",
    "#             self.conv1 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "#                 inputs=self.imageIn,num_outputs=16,\n",
    "#                 kernel_size=[8,8],stride=[4,4],padding='VALID')\n",
    "#             self.conv2 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "#                 inputs=self.conv1,num_outputs=32,\n",
    "#                 kernel_size=[4,4],stride=[2,2],padding='VALID')\n",
    "#             hidden = slim.fully_connected(slim.flatten(self.conv2),256,activation_fn=tf.nn.elu)\n",
    "            \n",
    "#             #Recurrent network for temporal dependencies\n",
    "#             lstm_cell = tf.contrib.rnn.BasicLSTMCell(256,state_is_tuple=True)\n",
    "#             c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n",
    "#             h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n",
    "#             self.state_init = [c_init, h_init]\n",
    "#             c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n",
    "#             h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n",
    "#             self.state_in = (c_in, h_in)\n",
    "#             rnn_in = tf.expand_dims(hidden, [0])\n",
    "#             step_size = tf.shape(self.imageIn)[:1]\n",
    "#             state_in = tf.contrib.rnn.LSTMStateTuple(c_in, h_in)\n",
    "#             lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "#                 lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n",
    "#                 time_major=False)\n",
    "#             lstm_c, lstm_h = lstm_state\n",
    "#             self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "#             rnn_out = tf.reshape(lstm_outputs, [-1, 256])\n",
    "            \n",
    "#             #Output layers for policy and value estimations\n",
    "#             self.policy = slim.fully_connected(rnn_out,a_size,\n",
    "#                 activation_fn=tf.nn.softmax,\n",
    "#                 weights_initializer=normalized_columns_initializer(0.01),\n",
    "#                 biases_initializer=None)\n",
    "#             self.value = slim.fully_connected(rnn_out,1,\n",
    "#                 activation_fn=None,\n",
    "#                 weights_initializer=normalized_columns_initializer(1.0),\n",
    "#                 biases_initializer=None)\n",
    "            \n",
    "#             #Only the worker network need ops for loss functions and gradient updating.\n",
    "#             if scope != 'global':\n",
    "#                 self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "#                 self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "#                 self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "#                 self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "\n",
    "#                 self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "#                 #Loss functions\n",
    "#                 self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "#                 self.entropy = - tf.reduce_sum(self.policy * tf.log(self.policy))\n",
    "#                 self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs)*self.advantages)\n",
    "#                 self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
    "\n",
    "#                 #Get gradients from local network using local losses\n",
    "#                 local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "#                 self.gradients = tf.gradients(self.loss,local_vars)\n",
    "#                 self.var_norms = tf.global_norm(local_vars)\n",
    "#                 grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,40.0)\n",
    "                \n",
    "#                 #Apply local gradients to global network\n",
    "#                 global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "#                 self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class A3CPlayer(BasePokerPlayer):\n",
    "    '''\n",
    "    DQN Player, bot wich using Double-Dueling-DQN architecture.\n",
    "\n",
    "    Parametrs\n",
    "    ---------\n",
    "    h_size : shape of layer after conv part (also before double part too)\n",
    "\n",
    "    lr : learning rate of the optimizer\n",
    "\n",
    "    gradient_clip_norm : gradients of the loss function will be clipped by this value\n",
    "    \n",
    "    total_num_actions : the number of actions witch agent can choose\n",
    "\n",
    "    is_double : whether or not to use the double architecture\n",
    "\n",
    "    is_main : whether or not to use this agent as main (when using the dueling architecture)\n",
    "\n",
    "    is_restore : wheter or not to use pretrained weight of the network\n",
    "\n",
    "    is_train : whether or not to use this agent for training\n",
    "\n",
    "    is_debug  wheter or not to print the debug information\n",
    "    '''\n",
    "    def __init__(self, a_size, scope, trainer, h_size=64):\n",
    "        self.h_size = h_size\n",
    "        \n",
    "#         with tf.variable_scope(scope):\n",
    "#             #Input and visual encoding layers\n",
    "#             self.scalar_input = tf.placeholder(tf.float32, [None, 17 * 17 * 1])\n",
    "#             self.features_input = tf.placeholder(tf.float32, [None, 20])\n",
    "\n",
    "#             xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "#             self.img_in = tf.reshape(self.scalar_input, [-1, 17, 17, 1])\n",
    "#             self.conv1 = tf.layers.conv2d(self.img_in, 32, 5, 2, activation=tf.nn.elu,\n",
    "#                                           kernel_initializer=xavier_init)\n",
    "#             self.conv2 = tf.layers.conv2d(self.conv1, 32, 3, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "#             self.conv3 = tf.layers.conv2d(self.conv2, self.h_size, 5, activation=tf.nn.elu,\n",
    "#                                           kernel_initializer=xavier_init)\n",
    "#             self.conv3_flat = tf.contrib.layers.flatten(self.conv3)\n",
    "# #             self.conv3_flat = tf.layers.dropout(self.conv3_flat)\n",
    "\n",
    "#             self.d1 = tf.layers.dense(self.features_input, 32, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "# #             self.d1 = tf.layers.dropout(self.d1)\n",
    "#             self.d2 = tf.layers.dense(self.d1, self.h_size, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "# #             self.d2 = tf.layers.dropout(self.d2)\n",
    "\n",
    "#             self.merge = tf.concat([self.conv3_flat, self.d2], axis=1)\n",
    "#             self.d3 = tf.layers.dense(self.merge, self.h_size, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "# #             self.d3 = tf.layers.dropout(self.d3)\n",
    "#             self.d4 = tf.layers.dense(self.d3, self.h_size, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "        \n",
    "# #             self.inputs = tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
    "# #             self.imageIn = tf.reshape(self.inputs,shape=[-1,84,84,1])\n",
    "# #             self.conv1 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "# #                 inputs=self.imageIn,num_outputs=16,\n",
    "# #                 kernel_size=[8,8],stride=[4,4],padding='VALID')\n",
    "# #             self.conv2 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "# #                 inputs=self.conv1,num_outputs=32,\n",
    "# #                 kernel_size=[4,4],stride=[2,2],padding='VALID')\n",
    "# #             hidden = slim.fully_connected(slim.flatten(self.conv2),256,activation_fn=tf.nn.elu)\n",
    "            \n",
    "#             #Recurrent network for temporal dependencies\n",
    "# #             lstm_cell = tf.contrib.rnn.BasicLSTMCell(256,state_is_tuple=True)\n",
    "# #             c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n",
    "# #             h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n",
    "# #             self.state_init = [c_init, h_init]\n",
    "# #             c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n",
    "# #             h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n",
    "# #             self.state_in = (c_in, h_in)\n",
    "# #             rnn_in = tf.expand_dims(hidden, [0])\n",
    "# #             step_size = tf.shape(self.imageIn)[:1]\n",
    "# #             state_in = tf.contrib.rnn.LSTMStateTuple(c_in, h_in)\n",
    "# #             lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "# #                 lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n",
    "# #                 time_major=False)\n",
    "# #             lstm_c, lstm_h = lstm_state\n",
    "# #             self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "# #             rnn_out = tf.reshape(lstm_outputs, [-1, 256])\n",
    "            \n",
    "#             #Output layers for policy and value estimations\n",
    "#             self.policy = tf.layers.dense(self.d4, a_size,\n",
    "#                 activation=tf.nn.softmax,\n",
    "#                 kernel_initializer=normalized_columns_initializer(0.01))\n",
    "#             self.value = tf.layers.dense(self.d4, 1,\n",
    "#                 activation=None,\n",
    "#                 kernel_initializer=normalized_columns_initializer(1.0))\n",
    "            \n",
    "#             #Only the worker network need ops for loss functions and gradient updating.\n",
    "#             if scope != 'global':\n",
    "#                 self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "#                 self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "#                 self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "#                 self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "\n",
    "#                 self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "#                 #Loss functions\n",
    "#                 self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "#                 self.entropy = - tf.reduce_sum(self.policy * tf.log(self.policy))\n",
    "#                 self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs)*self.advantages)\n",
    "#                 self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
    "\n",
    "#                 #Get gradients from local network using local losses\n",
    "#                 local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "#                 self.gradients = tf.gradients(self.loss,local_vars)\n",
    "#                 self.var_norms = tf.global_norm(local_vars)\n",
    "#                 grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,100.0)\n",
    "# #                 self.grad_norms = tf.global_norm(grads)\n",
    "                \n",
    "#                 #Apply local gradients to global network\n",
    "#                 global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "#                 self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))\n",
    "                \n",
    "    def _print(self, *msg):\n",
    "        if self.debug:\n",
    "            print(msg)\n",
    "        \n",
    "    def declare_action(self, valid_actions, hole_card, round_state):\n",
    "        street = round_state['street']\n",
    "        bank = round_state['pot']['main']['amount']\n",
    "        stack = [s['stack'] for s in round_state['seats'] if s['uuid'] == self.uuid][0]\n",
    "        other_stacks = [s['stack'] for s in round_state['seats'] if s['uuid'] != self.uuid]\n",
    "        dealer_btn = round_state['dealer_btn']\n",
    "        small_blind_pos = round_state['small_blind_pos']\n",
    "        big_blind_pos = round_state['big_blind_pos']\n",
    "        next_player = round_state['next_player']\n",
    "        round_count = round_state['round_count']\n",
    "        estimation = self.hole_card_est[(hole_card[0], hole_card[1])]\n",
    "\n",
    "        \n",
    "        self.features = get_street(street)\n",
    "        self.features.extend([bank, stack, dealer_btn, small_blind_pos, big_blind_pos, next_player, round_count])\n",
    "        self.features.extend(other_stacks)\n",
    "        self.features.append(estimation)\n",
    "        \n",
    "        img_state = img_from_state(hole_card, round_state)\n",
    "        img_state = process_img(img_state)\n",
    "        action_num = self.sess.run(self.predict, feed_dict={self.scalar_input: [img_state],\n",
    "                                                            self.features_input: [self.features]})[0]\n",
    "        qs = self.sess.run(self.Q_out, feed_dict={self.scalar_input: [img_state],\n",
    "                                                  self.features_input: [self.features]})[0]\n",
    "        self._print(qs)\n",
    "        action, amount = get_action_by_num(action_num, valid_actions)                    \n",
    "\n",
    "#         if not self.debug and np.random.rand() < 0.2:\n",
    "#             self.action_num = np.random.randint(0, 5)\n",
    "        return action, amount\n",
    "        \n",
    "    def receive_game_start_message(self, game_info):\n",
    "        pass\n",
    "    \n",
    "    def receive_round_start_message(self, round_count, hole_card, seats):\n",
    "        self._print(['Hole:', hole_card])        \n",
    "        self.start_stack = [s['stack'] for s in seats if s['uuid'] == self.uuid][0]\n",
    "        self._print(['Start stack:', self.start_stack])\n",
    "        estimation = self.hole_card_est[(hole_card[0], hole_card[1])]\n",
    "        self._print(['Estimation:', estimation])\n",
    "    \n",
    "    def receive_street_start_message(self, street, round_state):\n",
    "        pass\n",
    "            \n",
    "    def receive_game_update_message(self, action, round_state):\n",
    "        pass\n",
    "    \n",
    "    def receive_round_result_message(self, winners, hand_info, round_state):\n",
    "        end_stack = [s['stack'] for s in round_state['seats'] if s['uuid'] == self.uuid][0]\n",
    "        self._print(['End stack:', end_stack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self, a_size, scope, trainer, h_size=64):\n",
    "        self.h_size = h_size\n",
    "        \n",
    "        with tf.variable_scope(scope):\n",
    "            #Input and visual encoding layers\n",
    "            self.scalar_input = tf.placeholder(tf.float32, [None, 17 * 17 * 1])\n",
    "            self.features_input = tf.placeholder(tf.float32, [None, 20])\n",
    "\n",
    "            xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            self.img_in = tf.reshape(self.scalar_input, [-1, 17, 17, 1])\n",
    "            self.conv1 = tf.layers.conv2d(self.img_in, 32, 5, 2, activation=tf.nn.elu,\n",
    "                                          kernel_initializer=xavier_init)\n",
    "            self.conv2 = tf.layers.conv2d(self.conv1, 32, 3, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "            self.conv3 = tf.layers.conv2d(self.conv2, self.h_size, 5, activation=tf.nn.elu,\n",
    "                                          kernel_initializer=xavier_init)\n",
    "            self.conv3_flat = tf.contrib.layers.flatten(self.conv3)\n",
    "    #             self.conv3_flat = tf.layers.dropout(self.conv3_flat)\n",
    "\n",
    "            self.d1 = tf.layers.dense(self.features_input, 32, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "    #             self.d1 = tf.layers.dropout(self.d1)\n",
    "            self.d2 = tf.layers.dense(self.d1, self.h_size, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "    #             self.d2 = tf.layers.dropout(self.d2)\n",
    "\n",
    "            self.merge = tf.concat([self.conv3_flat, self.d2], axis=1)\n",
    "            self.d3 = tf.layers.dense(self.merge, self.h_size, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "    #             self.d3 = tf.layers.dropout(self.d3)\n",
    "            self.d4 = tf.layers.dense(self.d3, self.h_size, activation=tf.nn.elu, kernel_initializer=xavier_init)\n",
    "\n",
    "    #             self.inputs = tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
    "    #             self.imageIn = tf.reshape(self.inputs,shape=[-1,84,84,1])\n",
    "    #             self.conv1 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "    #                 inputs=self.imageIn,num_outputs=16,\n",
    "    #                 kernel_size=[8,8],stride=[4,4],padding='VALID')\n",
    "    #             self.conv2 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "    #                 inputs=self.conv1,num_outputs=32,\n",
    "    #                 kernel_size=[4,4],stride=[2,2],padding='VALID')\n",
    "    #             hidden = slim.fully_connected(slim.flatten(self.conv2),256,activation_fn=tf.nn.elu)\n",
    "\n",
    "            #Recurrent network for temporal dependencies\n",
    "    #             lstm_cell = tf.contrib.rnn.BasicLSTMCell(256,state_is_tuple=True)\n",
    "    #             c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n",
    "    #             h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n",
    "    #             self.state_init = [c_init, h_init]\n",
    "    #             c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n",
    "    #             h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n",
    "    #             self.state_in = (c_in, h_in)\n",
    "    #             rnn_in = tf.expand_dims(hidden, [0])\n",
    "    #             step_size = tf.shape(self.imageIn)[:1]\n",
    "    #             state_in = tf.contrib.rnn.LSTMStateTuple(c_in, h_in)\n",
    "    #             lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "    #                 lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n",
    "    #                 time_major=False)\n",
    "    #             lstm_c, lstm_h = lstm_state\n",
    "    #             self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "    #             rnn_out = tf.reshape(lstm_outputs, [-1, 256])\n",
    "\n",
    "            #Output layers for policy and value estimations\n",
    "            self.policy = tf.layers.dense(self.d4, a_size,\n",
    "                activation=tf.nn.softmax,\n",
    "                kernel_initializer=normalized_columns_initializer(0.01))\n",
    "            self.value = tf.layers.dense(self.d4, 1,\n",
    "                activation=None,\n",
    "                kernel_initializer=normalized_columns_initializer(1.0))\n",
    "\n",
    "            #Only the worker network need ops for loss functions and gradient updating.\n",
    "            if scope != 'global':\n",
    "                self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "                self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "                self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "\n",
    "                self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "                #Loss functions\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "                self.entropy = - tf.reduce_sum(self.policy * tf.log(self.policy))\n",
    "                self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs)*self.advantages)\n",
    "                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
    "\n",
    "                #Get gradients from local network using local losses\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss,local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,100.0)\n",
    "    #                 self.grad_norms = tf.global_norm(grads)\n",
    "\n",
    "                #Apply local gradients to global network\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# AC_Network(3, 'f', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_emul(my_uuid_):\n",
    "    global my_uuid\n",
    "    my_uuid = my_uuid_\n",
    "\n",
    "#     emul.register_player(\"1\", pm.CallPlayer())\n",
    "#     emul.register_player(\"2\", pm.CallPlayer())\n",
    "#     emul.register_player(\"3\", pm.CallPlayer())\n",
    "#     emul.register_player(\"4\", pm.CallPlayer())\n",
    "#     emul.register_player(\"5\", pm.CallPlayer())\n",
    "#     emul.register_player(\"6\", pm.CallPlayer())\n",
    "#     emul.register_player(\"7\", pm.CallPlayer())\n",
    "#     emul.register_player(\"8\", pm.CallPlayer())\n",
    "#     emul.register_player(\"9\", pm.CallPlayer())\n",
    "\n",
    "    emul.register_player(\"1\", pm.CallPlayer())\n",
    "    emul.register_player(\"2\", pm.CallPlayer())\n",
    "    emul.register_player(\"3\", pm.FoldPlayer())\n",
    "    emul.register_player(\"4\", pm.FoldPlayer())\n",
    "    emul.register_player(\"5\", pm.HeuristicPlayer())\n",
    "    emul.register_player(\"6\", pm.HeuristicPlayer())\n",
    "    emul.register_player(\"7\", pm.RandomPlayer())\n",
    "    emul.register_player(\"8\", pm.RandomPlayer())\n",
    "    emul.register_player(\"9\", pm.CallPlayer())\n",
    "\n",
    "\n",
    "    players_info = {\n",
    "        \"1\": { \"name\": \"CallPlayer1\", \"stack\": 1500 },\n",
    "        \"2\": { \"name\": \"CallPlayer2\", \"stack\": 1500 },\n",
    "        \"3\": { \"name\": \"FoldPlayer1\", \"stack\": 1500 },\n",
    "        \"4\": { \"name\": \"FoldPlayer2\", \"stack\": 1500 },\n",
    "        \"5\": { \"name\": \"HeuristicPlayer1\", \"stack\": 1500 },\n",
    "        \"6\": { \"name\": \"HeuristicPlayer2\", \"stack\": 1500 },\n",
    "        \"7\": { \"name\": \"RandomPlayer1\", \"stack\": 1500 },\n",
    "        \"8\": { \"name\": \"RandomPlayer2\", \"stack\": 1500 },\n",
    "        \"9\": { \"name\": \"DQN\", \"stack\": 1500 }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self, name, a_size, trainer, model_path, global_episodes):    \n",
    "        with open('../cache/hole_card_estimation.pkl', 'rb') as f:\n",
    "            self.hole_card_est = pickle.load(f)\n",
    "            \n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name        \n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.summary.FileWriter(\"../log/A3C/train_\"+str(self.number))\n",
    "\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_AC = AC_Network(a_size,self.name,trainer)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)    \n",
    "        \n",
    "        emul = MyEmulator()\n",
    "        emul.set_game_rule(9, 50, 15, 0)\n",
    "        self.my_uuid = '9'\n",
    "        self.players_info = {\n",
    "            \"1\": { \"name\": \"f1\", \"stack\": 1500 },\n",
    "            \"2\": { \"name\": \"f2\", \"stack\": 1500 },\n",
    "            \"3\": { \"name\": \"f3\", \"stack\": 1500 },\n",
    "            \"4\": { \"name\": \"f4\", \"stack\": 1500 },\n",
    "            \"5\": { \"name\": \"f5\", \"stack\": 1500 },\n",
    "            \"6\": { \"name\": \"f6\", \"stack\": 1500 },\n",
    "            \"7\": { \"name\": \"f7\", \"stack\": 1500 },\n",
    "            \"8\": { \"name\": \"f8\", \"stack\": 1500 },\n",
    "            \"9\": { \"name\": \"f9\", \"stack\": 1500 }\n",
    "        }\n",
    "    \n",
    "        emul.register_player(\"1\", pm.CallPlayer())\n",
    "        emul.register_player(\"2\", pm.CallPlayer())\n",
    "        emul.register_player(\"3\", pm.FoldPlayer())\n",
    "        emul.register_player(\"4\", pm.FoldPlayer())\n",
    "        emul.register_player(\"5\", pm.HeuristicPlayer())\n",
    "        emul.register_player(\"6\", pm.HeuristicPlayer())\n",
    "        emul.register_player(\"7\", pm.RandomPlayer())\n",
    "        emul.register_player(\"8\", pm.RandomPlayer())\n",
    "        emul.register_player(\"9\", pm.CallPlayer())\n",
    "    \n",
    "        self.actions = self.actions = np.identity(a_size,dtype=bool).tolist()\n",
    "        self.env = emul\n",
    "        \n",
    "    def train(self,rollout,sess,gamma,bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        last_img_states = rollout[:,0]\n",
    "        last_features = rollout[:,1]\n",
    "        last_actions_num = rollout[:,2]\n",
    "        rewards = rollout[:,3]\n",
    "        img_states = rollout[:,4]\n",
    "        features = rollout[:,5]\n",
    "        values = rollout[:,7]\n",
    "        \n",
    "        # Here we take the rewards and values from the rollout, and use them to \n",
    "        # generate the advantage and discounted returns. \n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        \n",
    "        # \"discount\" done earlier\n",
    "#         self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "#         discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "#         advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "#         advantages = discount(advantages,gamma)\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        discounted_rewards = rewards\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        feed_dict = {self.local_AC.target_v:discounted_rewards,\n",
    "            self.local_AC.scalar_input:np.vstack(last_img_states),\n",
    "            self.local_AC.features_input:np.vstack(last_features),\n",
    "            self.local_AC.actions:last_actions_num,\n",
    "            self.local_AC.advantages:advantages}\n",
    "#             self.local_AC.state_in[0]:self.batch_rnn_state[0],\n",
    "#             self.local_AC.state_in[1]:self.batch_rnn_state[1]}\n",
    "        v_l,p_l,e_l,g_n,v_n, _ = sess.run([self.local_AC.value_loss, # self.batch_rnn_state,\n",
    "            self.local_AC.policy_loss,\n",
    "            self.local_AC.entropy,\n",
    "            self.local_AC.grad_norms,\n",
    "            self.local_AC.var_norms,\n",
    "#             self.local_AC.state_out,\n",
    "            self.local_AC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout),e_l / len(rollout), g_n, v_n\n",
    "        \n",
    "    def work(self,gamma,sess,coord,saver):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        total_steps = 0\n",
    "        print (\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():                 \n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                d = False\n",
    "                \n",
    "                initial_state = self.env.generate_initial_game_state(self.players_info)\n",
    "                msgs = []\n",
    "                game_state, events = self.env.start_new_round(initial_state)\n",
    "                is_last_round = False\n",
    "#                 r_all = 0\n",
    "#                 j = 0\n",
    "\n",
    "                last_img_state = None\n",
    "                last_features = None\n",
    "                last_action_num = None\n",
    "                last_v = None\n",
    "                     \n",
    "#                 self.env.new_episode()\n",
    "#                 s = self.env.get_state().screen_buffer\n",
    "#                 episode_frames.append(s)\n",
    "#                 s = process_frame(s)\n",
    "#                 rnn_state = self.local_AC.state_init\n",
    "#                 self.batch_rnn_state = rnn_state\n",
    "#                 while self.env.is_episode_finished() == False:\n",
    "\n",
    "                round_buffer = []\n",
    "                while not is_last_round:\n",
    "                    #Take an action using probabilities from policy network output.\n",
    "                    a = self.env.run_until_my_next_action(game_state, self.my_uuid, msgs)\n",
    "                    \n",
    "                    if len(a) == 4:\n",
    "                        game_state, valid_actions, hole_card, round_state = a\n",
    "                        img_state = img_from_state(hole_card, round_state)\n",
    "                        img_state = process_img(img_state)\n",
    "\n",
    "                        street = round_state['street']\n",
    "                        bank = round_state['pot']['main']['amount']\n",
    "                        stack = [s['stack'] for s in round_state['seats'] if s['uuid'] == self.my_uuid][0]\n",
    "                        other_stacks = [s['stack'] for s in round_state['seats'] if s['uuid'] != self.my_uuid]\n",
    "                        dealer_btn = round_state['dealer_btn']\n",
    "                        small_blind_pos = round_state['small_blind_pos']\n",
    "                        big_blind_pos = round_state['big_blind_pos']\n",
    "                        next_player = round_state['next_player']\n",
    "                        round_count = round_state['round_count']\n",
    "                        estimation = self.hole_card_est[(hole_card[0], hole_card[1])]\n",
    "\n",
    "                        features = get_street(street)\n",
    "                        features.extend([bank, stack, dealer_btn, small_blind_pos, big_blind_pos, next_player,\n",
    "                                         round_count])\n",
    "                        features.extend(other_stacks)\n",
    "                        features.append(estimation)\n",
    "                     \n",
    "                        # add to buffer last hand \n",
    "                        if last_img_state is not None:\n",
    "                            round_buffer.append([last_img_state, last_features, last_action_num, 0, img_state,\n",
    "                                                   features, 0, last_v[0, 0]])\n",
    "                            episode_values.append(last_v[0, 0])\n",
    "                     \n",
    "                        pol_val = sess.run([self.local_AC.policy, self.local_AC.value],\n",
    "                                              feed_dict={self.local_AC.scalar_input: [img_state],\n",
    "                                                         self.local_AC.features_input: [features]})\n",
    "                        a_dist, v = pol_val[0], pol_val[1]\n",
    "\n",
    "                        a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                        a = np.argmax(a_dist == a)\n",
    "                        action, amount = get_action_by_num(a, valid_actions)\n",
    "                        game_state, msgs = self.env.apply_my_action(game_state, action, amount)\n",
    "\n",
    "                        last_img_state = img_state.copy()\n",
    "                        last_features = features.copy()\n",
    "                        last_action_num = a\n",
    "                        last_v = v\n",
    "                    else: # round end\n",
    "                        game_state, reward = a\n",
    "                        reward /= 1000\n",
    "                        episode_reward += reward\n",
    "\n",
    "#                         if reward >= 0:\n",
    "#                             reward = np.log(1 + reward)\n",
    "#                         else:\n",
    "#                             reward = -np.log(1 - reward)\n",
    "#                         r_all += reward\n",
    "\n",
    "                        # add to buffer last hand \n",
    "                        if last_img_state is not None:\n",
    "                            round_buffer.append([last_img_state, last_features, last_action_num, reward,\n",
    "                                                   last_img_state, last_features, 1, last_v[0, 0]])\n",
    "                            episode_values.append(last_v[0,0])\n",
    "\n",
    "                            # apply same reward for all states in round\n",
    "                            for k in range(len(round_buffer)):\n",
    "                                round_buffer[k][3] = reward\n",
    "                                \n",
    "                        episode_buffer.extend(round_buffer)\n",
    "                        round_buffer = []\n",
    "\n",
    "                        is_last_round = self.env._is_last_round(game_state, self.env.game_rule)\n",
    "                        game_state, events = self.env.start_new_round(game_state)\n",
    "\n",
    "                        last_img_state = None\n",
    "                        last_action_num = None   \n",
    "                        last_v = None\n",
    "                        \n",
    "                    self.episode_buffer = episode_buffer # for debug\n",
    "                    self.episode_values = episode_values\n",
    "#                     episode_buffer.append([s,a,r,s1,d,v[0,0]])\n",
    "#                     episode_values.append(v[0,0])\n",
    "\n",
    "#                     s = s1                    \n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "                    \n",
    "                    # If the episode hasn't ended, but the experience buffer is full, then we\n",
    "                    # make an update step using that experience rollout.\n",
    "#                     if len(episode_buffer) == 30 and d != True and episode_step_count != max_episode_length - 1:\n",
    "#                         # Since we don't know what the true final return is, we \"bootstrap\" from our current\n",
    "#                         # value estimation.\n",
    "#                         v1 = sess.run(self.local_AC.value, \n",
    "#                             feed_dict={self.local_AC.inputs:[s],\n",
    "#                             self.local_AC.state_in[0]:rnn_state[0],\n",
    "#                             self.local_AC.state_in[1]:rnn_state[1]})[0,0]\n",
    "#                         v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,v1)\n",
    "#                         episode_buffer = []\n",
    "#                         sess.run(self.update_local_ops)\n",
    "#                     if d == True:\n",
    "#                         break\n",
    "                                            \n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "                \n",
    "                # Update the network using the episode buffer at the end of the episode.\n",
    "                if len(episode_buffer) != 0:\n",
    "                    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
    "                                \n",
    "                    \n",
    "                if episode_count % 200 == 0 and self.name == 'worker_0':\n",
    "                    saver.save(sess, self.model_path, episode_count)\n",
    "                    print (\"Saved Model\", episode_count)\n",
    "                     \n",
    "                if episode_count % 1 == 0:\n",
    "                    mean_reward = np.mean(self.episode_rewards[-3:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-3:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-3:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Perf/Reward', simple_value=float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Length', simple_value=float(mean_length))\n",
    "                    summary.value.add(tag='Perf/Value', simple_value=float(mean_value))\n",
    "                    summary.value.add(tag='Losses/Value Loss', simple_value=float(v_l))\n",
    "                    summary.value.add(tag='Losses/Policy Loss', simple_value=float(p_l))\n",
    "                    summary.value.add(tag='Losses/Entropy', simple_value=float(e_l))\n",
    "                    summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                    summary.value.add(tag='Losses/Var Norm', simple_value=float(v_n))\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = .99 # discount rate for advantage estimation and reward discounting\n",
    "a_size = 5 # Agent can move Left, Right, or Fire\n",
    "load_model = False\n",
    "model_path = '../cache/models/A3C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting worker 0\n",
      "Starting worker 1\n",
      "Starting worker 2\n",
      "Starting worker 3\n",
      "Saved Model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-34:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-64-19e237ff1914>\", line 30, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma,sess,coord,saver)\n",
      "  File \"<ipython-input-62-43979b96175d>\", line 159, in work\n",
      "    self.local_AC.features_input: [features]})\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 778, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 914, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-19e237ff1914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mworker_threads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;31m# Wait for all threads to stop or for request_stop() to be called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mwait_for_stop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    309\u001b[0m       \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCoordinator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtold\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0mexpired\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mregister_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/digitman/miniconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-32:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-64-19e237ff1914>\", line 30, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma,sess,coord,saver)\n",
      "  File \"<ipython-input-62-43979b96175d>\", line 159, in work\n",
      "    self.local_AC.features_input: [features]})\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 778, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 914, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n",
      "Exception in thread Thread-33:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-64-19e237ff1914>\", line 30, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma,sess,coord,saver)\n",
      "  File \"<ipython-input-62-43979b96175d>\", line 159, in work\n",
      "    self.local_AC.features_input: [features]})\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 778, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/digitman/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 914, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    master_network = AC_Network(a_size,'global',None) # Generate global network\n",
    "    num_workers = multiprocessing.cpu_count() # Set workers ot number of available CPU threads\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(i,a_size,trainer,model_path,global_episodes))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    # This is where the asynchronous magic happens.\n",
    "    # Start the \"work\" process for each worker in a separate threat.\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.work(gamma,sess,coord,saver)\n",
    "        t = threading.Thread(target=(worker_work))\n",
    "        t.start()\n",
    "        sleep(0.5)\n",
    "        worker_threads.append(t)\n",
    "    coord.join(worker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
